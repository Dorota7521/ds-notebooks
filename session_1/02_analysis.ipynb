{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zajęcia 1 (część 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wczytanie danych\n",
    "\n",
    "Bedziemy wykorzystywać danye z ankiety StackOverflow z 2020.\n",
    "\n",
    "https://insights.stackoverflow.com/survey/\n",
    "\n",
    "Dane sa dostepne na google drive. Skorzystamy z modułu GoogleDriveDownloader, ktory pozwala pobrac dokument o podanym id.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: USER_ID=7021\n",
      "env: SEMESTER=2024l\n"
     ]
    }
   ],
   "source": [
    "# TODO Please update user id\n",
    "%env USER_ID=7021\n",
    "%env SEMESTER=2024l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: googledrivedownloader in /opt/conda/miniconda3/lib/python3.11/site-packages (0.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install googledrivedownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google_drive_downloader import GoogleDriveDownloader as gdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "path_dir = str(Path.home()) + \"/data/2020/\"  # ustawmy sciezke na HOME/data/2020\n",
    "archive_dir = path_dir + \"survey.zip\"        # plik zapiszemy pod nazwa survey.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/data/2020/'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/data/2020/survey.zip'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archive_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sciagniecie pliku we wskazane miejsce\n",
    "gdd.download_file_from_google_drive(file_id='1dfGerWeWkcyQ9GX9x20rdSGj7WtEpzBB',\n",
    "                                    dest_path=archive_dir,\n",
    "                                    unzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE 1 </span>\n",
    "Zapoznaj sie z plikami tekstowymi (survey_results_public.csv oraz survey_results_schema.csv). Podejrzyj ich zawartość, sprawdź ich wielkość (liczba linii oraz rozmiar). Wgraj plik do swojego kubełka `gs://ds-$SEMESTER-$USER_ID-notebook-data` na GCS do katalogu `survey/2020/`. Jesli nie masz kubełka stwórz go.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Podłączenie do sesji Spark na GKE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///root/data/2020/survey_results_public.csv [Content-Type=text/csv]...\n",
      "- [1 files][ 90.2 MiB/ 90.2 MiB]                                                \n",
      "Operation completed over 1 objects/90.2 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "! gsutil cp /root/data/2020/survey_results_public.csv gs://ds-$SEMESTER-$USER_ID-notebook-data/survey/2020/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WAZNE\n",
    "jesli w poprzednim notatniku masz aktywną sesję Spark zakończ ją (w poprzednim notatniku) poleceniem spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/22 15:28:06 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/12/22 15:28:06 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/12/22 15:28:06 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/12/22 15:28:06 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/12/22 15:28:08 ERROR ApplicationMaster: Uncaught exception: \n",
      "org.apache.hadoop.yarn.exceptions.InvalidApplicationMasterRequestException: Application doesn't exist in cache appattempt_1734872583941_0002_000001\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.throwApplicationDoesNotExistInCacheException(ApplicationMasterService.java:362)\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.registerApplicationMaster(ApplicationMasterService.java:260)\n",
      "\tat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.registerApplicationMaster(ApplicationMasterProtocolPBServiceImpl.java:90)\n",
      "\tat org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:101)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)\n",
      "\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)\n",
      "\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:?]\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:?]\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:?]\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490) ~[?:?]\n",
      "\tat org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat org.apache.hadoop.yarn.ipc.RPCUtil.instantiateYarnException(RPCUtil.java:75) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:116) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.registerApplicationMaster(ApplicationMasterProtocolPBClientImpl.java:110) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:?]\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:?]\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:?]\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566) ~[?:?]\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:433) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:166) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:158) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:96) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:362) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat com.sun.proxy.$Proxy41.registerApplicationMaster(Unknown Source) ~[?:?]\n",
      "\tat org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.registerApplicationMaster(AMRMClientImpl.java:247) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.registerApplicationMaster(AMRMClientImpl.java:234) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.registerApplicationMaster(AMRMClientImpl.java:214) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat org.apache.spark.deploy.yarn.YarnRMClient.register(YarnRMClient.scala:72) ~[spark-yarn_2.12-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.deploy.yarn.ApplicationMaster.registerAM(ApplicationMaster.scala:432) [spark-yarn_2.12-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.deploy.yarn.ApplicationMaster.runUnmanaged(ApplicationMaster.scala:307) [spark-yarn_2.12-3.5.1.jar:3.5.1]\n",
      "\tat org.apache.spark.deploy.yarn.Client$$anon$3.run(Client.scala:1221) [spark-yarn_2.12-3.5.1.jar:3.5.1]\n",
      "Caused by: org.apache.hadoop.ipc.RemoteException: Application doesn't exist in cache appattempt_1734872583941_0002_000001\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.throwApplicationDoesNotExistInCacheException(ApplicationMasterService.java:362)\n",
      "\tat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.registerApplicationMaster(ApplicationMasterService.java:260)\n",
      "\tat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.registerApplicationMaster(ApplicationMasterProtocolPBServiceImpl.java:90)\n",
      "\tat org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:101)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:621)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:589)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)\n",
      "\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)\n",
      "\tat java.base/java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)\n",
      "\n",
      "\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1566) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1512) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1409) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:258) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:139) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\tat com.sun.proxy.$Proxy40.registerApplicationMaster(Unknown Source) ~[?:?]\n",
      "\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.registerApplicationMaster(ApplicationMasterProtocolPBClientImpl.java:108) ~[hadoop-client-api-3.3.6.jar:?]\n",
      "\t... 17 more\n",
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      2\u001b[0m spark\u001b[38;5;241m.\u001b[39mstop()\n\u001b[1;32m      3\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.executor.instances\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspark.driver.memory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1g\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspark.executor.memory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1g\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/context.py:203\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/context.py:296\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/context.py:421\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1586\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1578\u001b[0m args_command \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m   1579\u001b[0m     [get_command_part(arg, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m new_args])\n\u001b[1;32m   1581\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1583\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1584\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1586\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1587\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1588\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fqn)\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "#spark.stop()\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".config(\"spark.executor.instances\", \"1\")\\\n",
    ".config('spark.driver.memory','1g')\\\n",
    ".config('spark.executor.memory', '1g') \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dostęp do danych na GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ścieżka dostępu do pliku na GCS\n",
    "# TODO Please update user id\n",
    "user_id = 7021\n",
    "semester = '2024l'\n",
    "gs_path = f'gs://ds-{semester}-{user_id}-notebook-data/survey/2020/survey_results_public.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://ds-2024l-7021-notebook-data/survey/2020/survey_results_public.csv'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL\n",
    "\n",
    "Platforma Apache Spark posiada komponent Spark SQL, który pozwala traktować dane jak tabele w bazie danych. Można zakładać swoje schematy baz danych oraz korzystać z języka SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table_name = \"survey_2020\"                               # nazwa tabeli ktora bedziemy chcieli stworzyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n",
      "24/12/22 15:18:17 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider csv. Persisting data source table `spark_catalog`.`default`.`survey_2020` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "24/12/22 15:18:18 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f'DROP TABLE IF EXISTS {table_name}')       # usun te tabele jesli istniala wczesniej \n",
    "\n",
    "# stworz tabele korzystajac z danych we wskazanej lokalizacji\n",
    "spark.sql(f'CREATE TABLE IF NOT EXISTS {table_name} \\\n",
    "          USING csv \\\n",
    "          OPTIONS (HEADER true, INFERSCHEMA true) \\\n",
    "          LOCATION \"{gs_path}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weryfikacja danych \n",
    "Sprawdzmy zaczytane dane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|          Respondent|      int|   NULL|\n",
      "|          MainBranch|   string|   NULL|\n",
      "|            Hobbyist|   string|   NULL|\n",
      "|                 Age|   string|   NULL|\n",
      "|          Age1stCode|   string|   NULL|\n",
      "|            CompFreq|   string|   NULL|\n",
      "|           CompTotal|   string|   NULL|\n",
      "|       ConvertedComp|   string|   NULL|\n",
      "|             Country|   string|   NULL|\n",
      "|        CurrencyDesc|   string|   NULL|\n",
      "|      CurrencySymbol|   string|   NULL|\n",
      "|DatabaseDesireNex...|   string|   NULL|\n",
      "|  DatabaseWorkedWith|   string|   NULL|\n",
      "|             DevType|   string|   NULL|\n",
      "|             EdLevel|   string|   NULL|\n",
      "|          Employment|   string|   NULL|\n",
      "|           Ethnicity|   string|   NULL|\n",
      "|              Gender|   string|   NULL|\n",
      "|          JobFactors|   string|   NULL|\n",
      "|              JobSat|   string|   NULL|\n",
      "+--------------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"describe {table_name}\").show() # nie wszystkie dane ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+---------+-------+\n",
      "|col_name                    |data_type|comment|\n",
      "+----------------------------+---------+-------+\n",
      "|Respondent                  |int      |NULL   |\n",
      "|MainBranch                  |string   |NULL   |\n",
      "|Hobbyist                    |string   |NULL   |\n",
      "|Age                         |string   |NULL   |\n",
      "|Age1stCode                  |string   |NULL   |\n",
      "|CompFreq                    |string   |NULL   |\n",
      "|CompTotal                   |string   |NULL   |\n",
      "|ConvertedComp               |string   |NULL   |\n",
      "|Country                     |string   |NULL   |\n",
      "|CurrencyDesc                |string   |NULL   |\n",
      "|CurrencySymbol              |string   |NULL   |\n",
      "|DatabaseDesireNextYear      |string   |NULL   |\n",
      "|DatabaseWorkedWith          |string   |NULL   |\n",
      "|DevType                     |string   |NULL   |\n",
      "|EdLevel                     |string   |NULL   |\n",
      "|Employment                  |string   |NULL   |\n",
      "|Ethnicity                   |string   |NULL   |\n",
      "|Gender                      |string   |NULL   |\n",
      "|JobFactors                  |string   |NULL   |\n",
      "|JobSat                      |string   |NULL   |\n",
      "|JobSeek                     |string   |NULL   |\n",
      "|LanguageDesireNextYear      |string   |NULL   |\n",
      "|LanguageWorkedWith          |string   |NULL   |\n",
      "|MiscTechDesireNextYear      |string   |NULL   |\n",
      "|MiscTechWorkedWith          |string   |NULL   |\n",
      "|NEWCollabToolsDesireNextYear|string   |NULL   |\n",
      "|NEWCollabToolsWorkedWith    |string   |NULL   |\n",
      "|NEWDevOps                   |string   |NULL   |\n",
      "|NEWDevOpsImpt               |string   |NULL   |\n",
      "|NEWEdImpt                   |string   |NULL   |\n",
      "|NEWJobHunt                  |string   |NULL   |\n",
      "|NEWJobHuntResearch          |string   |NULL   |\n",
      "|NEWLearn                    |string   |NULL   |\n",
      "|NEWOffTopic                 |string   |NULL   |\n",
      "|NEWOnboardGood              |string   |NULL   |\n",
      "|NEWOtherComms               |string   |NULL   |\n",
      "|NEWOvertime                 |string   |NULL   |\n",
      "|NEWPurchaseResearch         |string   |NULL   |\n",
      "|NEWPurpleLink               |string   |NULL   |\n",
      "|NEWSOSites                  |string   |NULL   |\n",
      "|NEWStuck                    |string   |NULL   |\n",
      "|OpSys                       |string   |NULL   |\n",
      "|OrgSize                     |string   |NULL   |\n",
      "|PlatformDesireNextYear      |string   |NULL   |\n",
      "|PlatformWorkedWith          |string   |NULL   |\n",
      "|PurchaseWhat                |string   |NULL   |\n",
      "|Sexuality                   |string   |NULL   |\n",
      "|SOAccount                   |string   |NULL   |\n",
      "|SOComm                      |string   |NULL   |\n",
      "|SOPartFreq                  |string   |NULL   |\n",
      "|SOVisitFreq                 |string   |NULL   |\n",
      "|SurveyEase                  |string   |NULL   |\n",
      "|SurveyLength                |string   |NULL   |\n",
      "|Trans                       |string   |NULL   |\n",
      "|UndergradMajor              |string   |NULL   |\n",
      "|WebframeDesireNextYear      |string   |NULL   |\n",
      "|WebframeWorkedWith          |string   |NULL   |\n",
      "|WelcomeChange               |string   |NULL   |\n",
      "|WorkWeekHrs                 |string   |NULL   |\n",
      "|YearsCode                   |string   |NULL   |\n",
      "|YearsCodePro                |string   |NULL   |\n",
      "+----------------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"describe {table_name}\").show(100, truncate=False) # niepoprawne typy danych... \"NA\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|Age|\n",
      "+---+\n",
      "| NA|\n",
      "| 99|\n",
      "| 98|\n",
      "| 97|\n",
      "| 96|\n",
      "| 95|\n",
      "| 94|\n",
      "| 89|\n",
      "| 88|\n",
      "| 86|\n",
      "| 85|\n",
      "| 84|\n",
      "| 83|\n",
      "| 81|\n",
      "| 80|\n",
      "| 79|\n",
      "| 78|\n",
      "| 77|\n",
      "| 76|\n",
      "| 75|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"SELECT DISTINCT Age FROM {table_name} ORDER BY Age DESC\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obsługa wartosci 'NA' - ponowne stworzenie tabeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/22 15:19:30 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider csv. Persisting data source table `spark_catalog`.`default`.`survey_2020` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f'DROP TABLE IF EXISTS {table_name}')\n",
    "\n",
    "# wykorzystujemy dodatkową opcję: NULLVALUE \"NA\"\n",
    "spark.sql(f'CREATE TABLE IF NOT EXISTS {table_name} \\\n",
    "          USING csv \\\n",
    "          OPTIONS (HEADER true, INFERSCHEMA true, NULLVALUE \"NA\") \\\n",
    "          LOCATION \"{gs_path}\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|          Respondent|      int|   NULL|\n",
      "|          MainBranch|   string|   NULL|\n",
      "|            Hobbyist|   string|   NULL|\n",
      "|                 Age|   double|   NULL|\n",
      "|          Age1stCode|   string|   NULL|\n",
      "|            CompFreq|   string|   NULL|\n",
      "|           CompTotal|   double|   NULL|\n",
      "|       ConvertedComp|   double|   NULL|\n",
      "|             Country|   string|   NULL|\n",
      "|        CurrencyDesc|   string|   NULL|\n",
      "|      CurrencySymbol|   string|   NULL|\n",
      "|DatabaseDesireNex...|   string|   NULL|\n",
      "|  DatabaseWorkedWith|   string|   NULL|\n",
      "|             DevType|   string|   NULL|\n",
      "|             EdLevel|   string|   NULL|\n",
      "|          Employment|   string|   NULL|\n",
      "|           Ethnicity|   string|   NULL|\n",
      "|              Gender|   string|   NULL|\n",
      "|          JobFactors|   string|   NULL|\n",
      "|              JobSat|   string|   NULL|\n",
      "|             JobSeek|   string|   NULL|\n",
      "|LanguageDesireNex...|   string|   NULL|\n",
      "|  LanguageWorkedWith|   string|   NULL|\n",
      "|MiscTechDesireNex...|   string|   NULL|\n",
      "|  MiscTechWorkedWith|   string|   NULL|\n",
      "|NEWCollabToolsDes...|   string|   NULL|\n",
      "|NEWCollabToolsWor...|   string|   NULL|\n",
      "|           NEWDevOps|   string|   NULL|\n",
      "|       NEWDevOpsImpt|   string|   NULL|\n",
      "|           NEWEdImpt|   string|   NULL|\n",
      "|          NEWJobHunt|   string|   NULL|\n",
      "|  NEWJobHuntResearch|   string|   NULL|\n",
      "|            NEWLearn|   string|   NULL|\n",
      "|         NEWOffTopic|   string|   NULL|\n",
      "|      NEWOnboardGood|   string|   NULL|\n",
      "|       NEWOtherComms|   string|   NULL|\n",
      "|         NEWOvertime|   string|   NULL|\n",
      "| NEWPurchaseResearch|   string|   NULL|\n",
      "|       NEWPurpleLink|   string|   NULL|\n",
      "|          NEWSOSites|   string|   NULL|\n",
      "|            NEWStuck|   string|   NULL|\n",
      "|               OpSys|   string|   NULL|\n",
      "|             OrgSize|   string|   NULL|\n",
      "|PlatformDesireNex...|   string|   NULL|\n",
      "|  PlatformWorkedWith|   string|   NULL|\n",
      "|        PurchaseWhat|   string|   NULL|\n",
      "|           Sexuality|   string|   NULL|\n",
      "|           SOAccount|   string|   NULL|\n",
      "|              SOComm|   string|   NULL|\n",
      "|          SOPartFreq|   string|   NULL|\n",
      "|         SOVisitFreq|   string|   NULL|\n",
      "|          SurveyEase|   string|   NULL|\n",
      "|        SurveyLength|   string|   NULL|\n",
      "|               Trans|   string|   NULL|\n",
      "|      UndergradMajor|   string|   NULL|\n",
      "|WebframeDesireNex...|   string|   NULL|\n",
      "|  WebframeWorkedWith|   string|   NULL|\n",
      "|       WelcomeChange|   string|   NULL|\n",
      "|         WorkWeekHrs|   double|   NULL|\n",
      "|           YearsCode|   string|   NULL|\n",
      "|        YearsCodePro|   string|   NULL|\n",
      "+--------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE {table_name}\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|  Age|\n",
      "+-----+\n",
      "|279.0|\n",
      "| 99.0|\n",
      "| 98.0|\n",
      "| 97.0|\n",
      "| 96.0|\n",
      "| 95.0|\n",
      "| 94.0|\n",
      "| 89.0|\n",
      "| 88.0|\n",
      "| 86.0|\n",
      "| 85.0|\n",
      "| 84.0|\n",
      "| 83.0|\n",
      "| 81.0|\n",
      "| 80.0|\n",
      "| 79.0|\n",
      "| 78.0|\n",
      "| 77.0|\n",
      "| 76.0|\n",
      "| 75.0|\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"SELECT DISTINCT  Age FROM {table_name} ORDER BY Age DESC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   64461|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# sprawdzenie liczności tabeli\n",
    "spark.sql(f\"select count(*) from {table_name}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[], functions=[count(1)])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=211]\n",
      "      +- HashAggregate(keys=[], functions=[partial_count(1)])\n",
      "         +- FileScan csv spark_catalog.default.survey_2020[] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[gs://ds-2024l-7021-notebook-data/survey/2020/survey_results_public.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select count(*) from {table_name}\").explain()  # tak jak na poprzednich zajeciach mozemy wygenerowac plany wykonania polecenia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podgląd danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/22 15:20:03 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------+----+----------+--------+---------+-------------+------------------+--------------------+--------------+----------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------+--------------------+--------------------+--------------------+----------------------+--------------------+----------------------+--------------------+----------------------------+------------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------+--------------------+-------------+--------------------+--------------------+-----------------+--------------------+--------------------+-----------+--------------------+----------------------+--------------------+--------------------+--------------------+---------+---------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+----------------------+--------------------+--------------------+-----------+---------+------------+\n",
      "|Respondent|          MainBranch|Hobbyist| Age|Age1stCode|CompFreq|CompTotal|ConvertedComp|           Country|        CurrencyDesc|CurrencySymbol|DatabaseDesireNextYear|  DatabaseWorkedWith|             DevType|             EdLevel|          Employment|           Ethnicity|Gender|          JobFactors|              JobSat|             JobSeek|LanguageDesireNextYear|  LanguageWorkedWith|MiscTechDesireNextYear|  MiscTechWorkedWith|NEWCollabToolsDesireNextYear|NEWCollabToolsWorkedWith|NEWDevOps|      NEWDevOpsImpt|           NEWEdImpt|          NEWJobHunt|  NEWJobHuntResearch|            NEWLearn|NEWOffTopic|      NEWOnboardGood|NEWOtherComms|         NEWOvertime| NEWPurchaseResearch|    NEWPurpleLink|          NEWSOSites|            NEWStuck|      OpSys|             OrgSize|PlatformDesireNextYear|  PlatformWorkedWith|        PurchaseWhat|           Sexuality|SOAccount|         SOComm|          SOPartFreq|         SOVisitFreq|          SurveyEase|        SurveyLength|Trans|      UndergradMajor|WebframeDesireNextYear|  WebframeWorkedWith|       WelcomeChange|WorkWeekHrs|YearsCode|YearsCodePro|\n",
      "+----------+--------------------+--------+----+----------+--------+---------+-------------+------------------+--------------------+--------------+----------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------+--------------------+--------------------+--------------------+----------------------+--------------------+----------------------+--------------------+----------------------------+------------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------+--------------------+-------------+--------------------+--------------------+-----------------+--------------------+--------------------+-----------+--------------------+----------------------+--------------------+--------------------+--------------------+---------+---------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+----------------------+--------------------+--------------------+-----------+---------+------------+\n",
      "|         1|I am a developer ...|     Yes|NULL|        13| Monthly|     NULL|         NULL|           Germany|       European Euro|           EUR|  Microsoft SQL Server|Elasticsearch;Mic...|Developer, deskto...|Master’s degree (...|Independent contr...|White or of Europ...|   Man|Languages, framew...|  Slightly satisfied|I am not interest...|  C#;HTML/CSS;JavaS...|C#;HTML/CSS;JavaS...|     .NET Core;Xamarin|      .NET;.NET Core|        Microsoft Teams;M...|    Confluence;Jira;S...|       No| Somewhat important|    Fairly important|                NULL|                NULL|         Once a year|   Not sure|                NULL|           No|Often: 1-2 days p...|Start a free tria...|           Amused|Stack Overflow (p...|Visit Stack Overf...|    Windows|    2 to 9 employees|  Android;iOS;Kuber...|             Windows|                NULL|Straight / Hetero...|       No| No, not at all|                NULL|Multiple times pe...|Neither easy nor ...|Appropriate in le...|   No|Computer science,...|          ASP.NET Core|ASP.NET;ASP.NET Core|Just as welcome n...|       50.0|       36|          27|\n",
      "|         2|I am a developer ...|      No|NULL|        19|    NULL|     NULL|         NULL|    United Kingdom|      Pound sterling|           GBP|                  NULL|                NULL|Developer, full-s...|Bachelor’s degree...|  Employed full-time|                NULL|  NULL|                NULL|   Very dissatisfied|I am not interest...|          Python;Swift|    JavaScript;Swift|  React Native;Tens...|        React Native|                Github;Slack|    Confluence;Jira;G...|     NULL|               NULL|    Fairly important|                NULL|                NULL|         Once a year|   Not sure|                NULL|           No|                NULL|                NULL|           Amused|Stack Overflow (p...|Visit Stack Overf...|      MacOS|1,000 to 4,999 em...|  iOS;Kubernetes;Li...|                 iOS|I have little or ...|                NULL|      Yes|Yes, definitely|Less than once pe...|Multiple times pe...|                NULL|                NULL| NULL|Computer science,...|                  NULL|                NULL|Somewhat more wel...|       NULL|        7|           4|\n",
      "|         3|I code primarily ...|     Yes|NULL|        15|    NULL|     NULL|         NULL|Russian Federation|                NULL|          NULL|                  NULL|                NULL|                NULL|                NULL|                NULL|                NULL|  NULL|                NULL|                NULL|                NULL|  Objective-C;Pytho...|Objective-C;Pytho...|                  NULL|                NULL|                        NULL|                    NULL|     NULL|               NULL|                NULL|                NULL|                NULL|       Once a decade|       NULL|                NULL|           No|                NULL|                NULL|             NULL|Stack Overflow (p...|                NULL|Linux-based|                NULL|                  NULL|                NULL|                NULL|                NULL|      Yes|  Yes, somewhat|A few times per m...|Daily or almost d...|Neither easy nor ...|Appropriate in le...| NULL|                NULL|                  NULL|                NULL|Somewhat more wel...|       NULL|        4|        NULL|\n",
      "|         4|I am a developer ...|     Yes|25.0|        18|    NULL|     NULL|         NULL|           Albania|        Albanian lek|           ALL|                  NULL|                NULL|                NULL|Master’s degree (...|                NULL|White or of Europ...|   Man|Flex time or a fl...|Slightly dissatis...|I’m not actively ...|                  NULL|                NULL|                  NULL|                NULL|                        NULL|                    NULL|       No|               NULL|Not at all import...|Curious about oth...|                NULL|         Once a year|   Not sure|                 Yes|          Yes|Occasionally: 1-2...|                NULL|             NULL|Stack Overflow (p...|                NULL|Linux-based|  20 to 99 employees|                  NULL|                NULL|I have a great de...|Straight / Hetero...|      Yes|Yes, definitely|A few times per m...|Multiple times pe...|                NULL|                NULL|   No|Computer science,...|                  NULL|                NULL|Somewhat less wel...|       40.0|        7|           4|\n",
      "|         5|I used to be a de...|     Yes|31.0|        16|    NULL|     NULL|         NULL|     United States|                NULL|          NULL|      MySQL;PostgreSQL|MySQL;PostgreSQL;...|                NULL|Bachelor’s degree...|  Employed full-time|White or of Europ...|   Man|                NULL|                NULL|                NULL|       Java;Ruby;Scala|   HTML/CSS;Ruby;SQL|          Ansible;Chef|             Ansible|        Github;Google Sui...|    Confluence;Jira;G...|     NULL|               NULL|      Very important|                NULL|                NULL|         Once a year|         No|                NULL|          Yes|                NULL|Start a free tria...|Hello, old friend|Stack Overflow (p...|Call a coworker o...|    Windows|                NULL|  Docker;Google Clo...|AWS;Docker;Linux;...|                NULL|Straight / Hetero...|      Yes|  Yes, somewhat|Less than once pe...|A few times per m...|                Easy|           Too short|   No|Computer science,...|  Django;Ruby on Rails|       Ruby on Rails|Just as welcome n...|       NULL|       15|           8|\n",
      "|         6|I am a developer ...|      No|NULL|        14|    NULL|     NULL|         NULL|           Germany|       European Euro|           EUR|                  NULL|                NULL|Designer;Develope...|Secondary school ...|  Employed full-time|White or of Europ...|   Man|Diversity of the ...|  Slightly satisfied|I am not interest...|  HTML/CSS;Java;Jav...|HTML/CSS;Java;Jav...|                  NULL|                NULL|                Github;Slack|    Confluence;Github...| Not sure|               NULL|    Fairly important|                NULL|                NULL|         Once a year|         No|                  No|           No|               Never|Ask developers I ...|           Amused|Stack Overflow (p...|Play games;Visit ...|    Windows|                NULL|               Android|Android;Docker;Wo...|I have some influ...|Straight / Hetero...|      Yes|  Yes, somewhat|A few times per m...|A few times per week|Neither easy nor ...|Appropriate in le...| NULL|                NULL|              React.js|                NULL|                NULL|       NULL|        6|           4|\n",
      "|         7|I am a developer ...|     Yes|NULL|        18| Monthly|     NULL|         NULL|             India|United States dollar|           USD|                  NULL|                NULL|Developer, back-e...|Bachelor’s degree...|  Employed full-time|                NULL|  NULL|                NULL|      Very satisfied|I’m not actively ...|       C#;HTML/CSS;PHP|     C#;HTML/CSS;PHP|                  NULL|                NULL|                        NULL|                    NULL|      Yes|Extremely important|      Very important|Better compensati...|Read company medi...|    Every few months|        Yes|                 Yes|           No|Sometimes: 1-2 da...|Start a free tria...|             NULL|Stack Overflow (p...|                NULL|    Windows|  20 to 99 employees|                  NULL|                NULL|                NULL|                NULL|      Yes|Yes, definitely|Multiple times pe...|Multiple times pe...|                NULL|                NULL| NULL|Computer science,...|                  NULL|                NULL|A lot more welcom...|       NULL|        6|           4|\n",
      "|         8|I am a developer ...|     Yes|36.0|        12|  Yearly| 116000.0|     116000.0|     United States|United States dollar|           USD|               MongoDB| MariaDB;MySQL;Redis|Developer, back-e...|Bachelor’s degree...|  Employed full-time|White or of Europ...|   Man|Remote work optio...|Slightly dissatis...|I’m not actively ...|            JavaScript|          Python;SQL|              Unity 3D|             Ansible|                        NULL|    Confluence;Jira;G...|      Yes|Extremely important|Not at all import...|Curious about oth...|Read company medi...|         Once a year|   Not sure|                 Yes|           No|Occasionally: 1-2...|Start a free tria...|Hello, old friend|Stack Overflow (p...|Play games;Call a...|Linux-based|  20 to 99 employees|  iOS;Slack Apps an...|              Docker|I have some influ...|Straight / Hetero...|      Yes| No, not really|Less than once pe...|Multiple times pe...|                Easy|Appropriate in le...|   No|Computer science,...|  Django;React.js;V...|               Flask|Just as welcome n...|       39.0|       17|          13|\n",
      "|         9|I am a developer ...|      No|30.0|        20|    NULL|     NULL|         NULL|           Tunisia|United States dollar|           USD|                  NULL|                NULL|Developer, full-s...|Professional degr...|Independent contr...|                NULL|   Man|Diversity of the ...|   Very dissatisfied|I’m not actively ...|           Python;Rust|HTML/CSS;JavaScri...|                  NULL|                NULL|                        NULL|    Github;Slack;Trel...|     NULL|               NULL|      Very important|Curious about oth...|                NULL|         Once a year|         No|                 Yes|           No|Occasionally: 1-2...|  Start a free trial|Hello, old friend|Stack Overflow (p...|            Meditate|    Windows|Just me - I am a ...|                  NULL|           WordPress|                NULL|Straight / Hetero...|      Yes|Yes, definitely|Multiple times pe...|Multiple times pe...|Neither easy nor ...|Appropriate in le...|   No|Computer science,...|            Angular.js|              jQuery|Just as welcome n...|       50.0|        6|           4|\n",
      "|        10|I am a developer ...|     Yes|22.0|        14|  Yearly|  25000.0|      32315.0|    United Kingdom|      Pound sterling|           GBP|  Microsoft SQL Server|Microsoft SQL Server|Database administ...|Master’s degree (...|  Employed full-time|White or of Europ...|   Man|Flex time or a fl...|      Very satisfied|I’m not actively ...|  HTML/CSS;Java;Jav...|HTML/CSS;Java;Jav...|     Pandas;TensorFlow|              Pandas|        Github;Microsoft ...|    Github;Microsoft ...|       No|            Neutral|    Fairly important|Curious about oth...|Company reviews f...|    Every few months|        Yes|Onboarding? What ...|           No|Often: 1-2 days p...|Start a free tria...|Hello, old friend|Stack Overflow (p...|Visit Stack Overflow|    Windows|    2 to 9 employees|  Android;Linux;Ras...|Android;Linux;Ras...|I have a great de...|Straight / Hetero...|      Yes|Yes, definitely|Multiple times pe...|Multiple times pe...|                Easy|Appropriate in le...|   No|Mathematics or st...|          Flask;jQuery|        Flask;jQuery|Somewhat more wel...|       36.0|        8|           4|\n",
      "|        11|I am a developer ...|     Yes|23.0|        13|  Yearly|  31000.0|      40070.0|    United Kingdom|      Pound sterling|           GBP|  Firebase;MongoDB;...|Firebase;MongoDB;...|Developer, back-e...|Bachelor’s degree...|  Employed full-time|White or of Europ...|   Man|Flex time or a fl...|Slightly dissatis...|I am actively loo...|  Go;JavaScript;Swi...| C#;JavaScript;Swift|  Node.js;React Native|             Node.js|        Jira;Github;Slack...|    Confluence;Jira;G...|       No|Extremely important|      Very important|Curious about oth...|Read company medi...|    Every few months|         No|                 Yes|           No|Rarely: 1-2 days ...|                NULL|          Annoyed|Stack Overflow (p...|Play games;Call a...|    Windows|10,000 or more em...|  AWS;Docker;iOS;MacOS|      AWS;Heroku;iOS|I have little or ...|Straight / Hetero...|      Yes|  Yes, somewhat|I have never part...|Multiple times pe...|                Easy|Appropriate in le...|   No|Computer science,...|  Angular;Django;Re...|Angular;Angular.j...|Just as welcome n...|       40.0|       10|           2|\n",
      "|        12|I am a developer ...|      No|49.0|        42| Monthly|   1100.0|      14268.0|             Spain|       European Euro|           EUR|                  NULL|                NULL|Designer;Develope...|Some college/univ...|  Employed full-time|White or of Europ...|   Man|Remote work optio...|   Very dissatisfied|I’m not actively ...|   HTML/CSS;JavaScript| HTML/CSS;JavaScript|                  .NET|                .NET|                      Github|                  Github| Not sure|            Neutral|  Somewhat important| Better compensation|Personal network ...|Once every few years|   Not sure|                  No|           No|Rarely: 1-2 days ...|                NULL|      Indifferent|Stack Overflow (p...|Visit Stack Overflow|    Windows|    2 to 9 employees|     Windows;WordPress|   Windows;WordPress|I have little or ...|Straight / Hetero...|      Yes|  Yes, somewhat|A few times per week|Multiple times pe...|                Easy|Appropriate in le...|   No|Mathematics or st...|        ASP.NET;jQuery|      ASP.NET;jQuery|Just as welcome n...|       40.0|        7|           7|\n",
      "|        13|I am not primaril...|     Yes|53.0|        14| Monthly|   3000.0|      38916.0|       Netherlands|       European Euro|           EUR|                  NULL|                NULL|Designer;Develope...|Secondary school ...|  Employed full-time|White or of Europ...|   Man|Industry that I’d...|      Very satisfied|I am not interest...|                Python| C;JavaScript;Python|                  NULL|                NULL|                        NULL|                    NULL|       No|               NULL|Not at all import...|Having a bad day ...|Read company medi...|Once every few years|         No|                 Yes|           No|Rarely: 1-2 days ...|  Start a free trial|           Amused|Stack Overflow (p...|Call a coworker o...|      MacOS|    2 to 9 employees|   Linux;MacOS;Windows|       MacOS;Windows|I have some influ...|Straight / Hetero...|      Yes|Yes, definitely|Multiple times pe...|Multiple times pe...|Neither easy nor ...|            Too long|   No|                NULL|                  NULL|                NULL|A lot less welcom...|       36.0|       35|          20|\n",
      "|        14|I am a developer ...|     Yes|27.0|        13|  Yearly|  66000.0|      66000.0|     United States|United States dollar|           USD|  Firebase;Microsof...|Firebase;Microsof...|Developer, deskto...|Associate degree ...|  Employed full-time|White or of Europ...|   Man|Industry that I’d...|  Slightly satisfied|I’m not actively ...|  HTML/CSS;JavaScri...|HTML/CSS;JavaScri...|               Node.js|             Node.js|        Confluence;Jira;G...|    Confluence;Jira;G...|      Yes|Extremely important|  Somewhat important|Wanting to share ...|Read company medi...|    Every few months|         No|                 Yes|           No|Occasionally: 1-2...|                NULL|           Amused|Stack Overflow (p...|Call a coworker o...|    Windows|100 to 499 employees|  Google Cloud Plat...|Google Cloud Plat...|I have little or ...|Straight / Hetero...|      Yes|Yes, definitely|A few times per week|Multiple times pe...|Neither easy nor ...|Appropriate in le...|   No|Computer science,...|        Angular;Vue.js|      Angular;Vue.js|Just as welcome n...|       40.0|        5|           1|\n",
      "|        15|I am a student wh...|     Yes|NULL|        13|    NULL|     NULL|         NULL|            France|                NULL|          NULL|                  NULL|        MySQL;Oracle|                NULL|Bachelor’s degree...|             Student|                NULL|   Man|Languages, framew...|                NULL|I’m not actively ...|  Assembly;Bash/She...|Bash/Shell/PowerS...|                  NULL|                NULL|               Github;Gitlab|                  Github|     NULL|               NULL|                NULL|                NULL|                NULL|Once every few years|        Yes|                NULL|           No|                NULL|Ask developers I ...|          Annoyed|Stack Overflow (p...|Play games;Visit ...|Linux-based|                NULL|                 Linux|               Linux|                NULL|      Gay or Lesbian|       No| No, not really|                NULL|A few times per m...|                Easy|Appropriate in le...|   No|Computer science,...|                  NULL|                NULL|Just as welcome n...|       NULL|        4|        NULL|\n",
      "|        16|I am a developer ...|     Yes|45.0|         8| Monthly|   7000.0|     108576.0|    United Kingdom|      Pound sterling|           GBP|                  NULL|          PostgreSQL|Database administ...|Bachelor’s degree...|Independent contr...|White or of Europ...|   Man|Industry that I’d...|      Very satisfied|I’m not actively ...|                    Go|Bash/Shell/PowerS...|                  NULL|                NULL|                        NULL|    Github;Slack;Goog...|       No|Extremely important|    Fairly important|Having a bad day ...|Personal network ...|         Once a year|   Not sure|                 Yes|           No|Often: 1-2 days p...|Ask developers I ...|Hello, old friend|Stack Overflow (p...|Visit Stack Overf...|      MacOS|Just me - I am a ...|                  NULL|AWS;Docker;Google...|                NULL|Straight / Hetero...|      Yes|  Yes, somewhat|Multiple times pe...|Multiple times pe...|                Easy|Appropriate in le...|   No|Computer science,...|                  NULL|     jQuery;React.js|Just as welcome n...|       50.0|       37|          23|\n",
      "|        17|I am a developer ...|     Yes|25.0|        14|  Yearly|  79000.0|      79000.0|     United States|United States dollar|           USD|  Cassandra;Elastic...|Microsoft SQL Ser...|Developer, full-s...|Bachelor’s degree...|  Employed full-time|White or of Europ...|   Man|Industry that I’d...|Slightly dissatis...|I’m not actively ...|  C#;Go;Haskell;HTM...|C#;HTML/CSS;JavaS...|    .NET Core;Teraform|        .NET;Node.js|        Github;Gitlab;Sla...|    Github;Gitlab;Sla...|      Yes|Extremely important|    Fairly important|Having a bad day ...|Company reviews f...|         Once a year|   Not sure|                  No|           No|Rarely: 1-2 days ...|                NULL|Hello, old friend|Stack Overflow (p...|Call a coworker o...|    Windows|  20 to 99 employees|  Docker;Kubernetes...|                NULL|I have little or ...|Straight / Hetero...|      Yes| No, not at all|I have never part...|Daily or almost d...|                Easy|Appropriate in le...|   No|Computer science,...|  ASP.NET Core;Gats...|ASP.NET;Gatsby;jQ...|Just as welcome n...|       40.0|        7|           3|\n",
      "|        18|I am a developer ...|     Yes|32.0|        12| Monthly| 105000.0|    1260000.0|     United States|United States dollar|           USD|  Elasticsearch;Pos...|Elasticsearch;Mar...| Developer, back-end|Bachelor’s degree...|  Employed full-time|White or of Europ...|   Man|Languages, framew...|      Very satisfied|I am not interest...|         HTML/CSS;Perl|Bash/Shell/PowerS...|                  NULL|                NULL|                      Github|            Github;Slack| Not sure|            Neutral|      Very important| Looking to relocate|                NULL|Once every few years|        Yes|Onboarding? What ...|          Yes|Occasionally: 1-2...|Start a free tria...|Hello, old friend|Stack Overflow (p...|Play games;Visit ...|Linux-based|100 to 499 employees|                 Linux|           AWS;Linux|I have some influ...|Straight / Hetero...|      Yes|  Yes, somewhat|I have never part...|Daily or almost d...|Neither easy nor ...|Appropriate in le...|   No|Computer science,...|                  NULL|                NULL|Just as welcome n...|       45.0|       19|          12|\n",
      "|        19|I am a developer ...|      No|24.0|        15|  Yearly|  83400.0|      83400.0|     United States|United States dollar|           USD|       MariaDB;MongoDB|MariaDB;Microsoft...|Developer, full-s...|Bachelor’s degree...|  Employed full-time|White or of Europ...|   Man|Flex time or a fl...|Slightly dissatis...|I’m not actively ...|  C#;Go;Python;Type...|Bash/Shell/PowerS...|  Node.js;React Nat...|.NET;.NET Core;No...|                Github;Slack|    Confluence;Jira;G...|      Yes|Extremely important|    Fairly important|Having a bad day ...|Company reviews f...|         Once a year|        Yes|                  No|           No|Occasionally: 1-2...|                NULL|Hello, old friend|Stack Overflow (p...|Call a coworker o...|    Windows|10,000 or more em...|  iOS;Slack Apps an...|             Windows|I have little or ...|Straight / Hetero...|      Yes|        Neutral|Less than once pe...|A few times per week|                Easy|Appropriate in le...|   No|Computer science,...|  Django;Express;Re...|Angular;Angular.j...|Just as welcome n...|       35.0|        9|           3|\n",
      "|        20|I am a developer ...|     Yes|40.0|        18|    NULL|     NULL|         NULL|             Spain|       European Euro|           EUR|                  NULL|IBM DB2;MariaDB;M...|Developer, deskto...|Bachelor’s degree...|  Employed full-time|Hispanic or Latin...|   Man|                NULL|Neither satisfied...|I’m not actively ...|                    Go| Java;JavaScript;SQL|            TensorFlow|             Node.js|                      Github|             Jira;Github|      Yes|            Neutral|      Very important|Having a bad day ...|Personal network ...|Once every few years|   Not sure|                  No|           No|Often: 1-2 days p...|                NULL|Hello, old friend|Stack Overflow (p...|Meditate;Call a c...|    Windows|100 to 499 employees|                  NULL|Heroku;Linux;Windows|I have little or ...|Straight / Hetero...|      Yes|  Yes, somewhat|A few times per m...|Multiple times pe...|                Easy|           Too short|   No|Another engineeri...|                Spring|   Angular.js;Spring|Just as welcome n...|       50.0|       22|          17|\n",
      "+----------+--------------------+--------+----+----------+--------+---------+-------------+------------------+--------------------+--------------+----------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------+--------------------+--------------------+--------------------+----------------------+--------------------+----------------------+--------------------+----------------------------+------------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------+--------------------+-------------+--------------------+--------------------+-----------------+--------------------+--------------------+-----------+--------------------+----------------------+--------------------+--------------------+--------------------+---------+---------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+----------------------+--------------------+--------------------+-----------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select * from {table_name}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biblioteka Pandas\n",
    "\n",
    "https://pandas.pydata.org/\n",
    "\n",
    "Moduł Pandas jest biblioteką Pythonową do manipulacji danymi. W szczegolnosci w pandas mozemy stworzyc ramki danych i wykonywac na niej analize, agregacje oraz wizualizacje danych. \n",
    "Przy nieduzych zbiorach danych i prostych operacjach to doskonała biblioteka. Jednak kiedy zbior danych sie rozrasta lub kiedy wymagane sa zlozone transformacje to operacje moga byc wolne.\n",
    "\n",
    "Operacje na rozproszonych danych sa szybsze. Ale tu takze napotykamy ograniczenia np trudność w wizualizacji danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select * from {table_name} limit 10\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ważne** \n",
    "\n",
    "Metoda toPandas() na ramce pyspark, konwertuje ramkę pyspark do ramki pandas. Wykonuje akcje pobrania (collect) wszystkich danych z executorów (z JVM) i transfer do  programu sterujacego (driver) i konwersje do typu Pythonowego w notatniku. Ze względu na ograniczenia pamięciowe w programie sterującym należy to wykonywać na podzbiorach danych.\n",
    "\n",
    "**DataFrame.collect() collects the distributed data to the driver side as the local data in Python. Note that this can throw an out-of-memory error when the dataset is too large to fit in the driver side because it collects all the data from executors to the driver side.**\n",
    "\n",
    "**Note that DataFrame.toPandas() results in the collection of all records in the DataFrame to the driver program and should be done on a small subset of the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_df = spark.sql(f\"select * from {table_name} LIMIT 10\")\n",
    "local_df = spark.sql(f\"select * from {table_name} LIMIT 10\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dist_df)  # dataframe Sparkowy (\"przepis na dane, rozproszony, leniwy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(local_df)  # dataframe Pandasowy (lokalny, sciągnięty do pamięci operacyjnej)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)    # pokazuj wszystkie kolumny\n",
    "# pd.reset_option(“display.max_columns”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE 2 </span>\n",
    "Napisz w Spark SQL zapytanie które zwróci średnią liczbę godzin przepracowywanych przez z respondentów pogrupowanych ze względu na kraj. Następnie przekształć wynik do ramki pandasowej i ją wyświetl.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wizualizacje\n",
    "\n",
    "Do wizualizacji będziemy się posługiwać modułami matplotlib (https://matplotlib.org/) i seaborn (https://seaborn.pydata.org/). Do bardzo rozbudowane moduły, zachęcamy do eksploracji oficjalnych dokumentacji. Na zajęciach zrealizujemy następujące wykresy:\n",
    "* histogramy\n",
    "* liniowe \n",
    "* wiolinowe\n",
    "* kołowe \n",
    "\n",
    "Moduły wizualizacyjne wymagają danych na lokalnej maszynie. Mogą być to natywne typy danych Pythonowe (słowniki, listy) ale także np ramki danych pandasowe. ~~Nie działa wizualizacja na ramkach danych Sparkowych.~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narysuj histogram wieku respondentów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przygotowanie danych\n",
    "# przycinamy dane tylko do zakresu ktory jest potrzebny do realizacji polecenia\n",
    "ages = spark.sql(f\"SELECT CAST (Age AS INT) \\\n",
    "                    FROM {table_name} \\\n",
    "                    WHERE age IS NOT NULL \\\n",
    "                    AND age BETWEEN 10 AND 80\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages.hist(\"Age\", bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(ages, bins=10, rug=True, kde=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaki jest udział programistów hobbistów? Narysuj wykres kołowy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Będzie nas interesowała ta proporcja ze względu na płeć."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przygotowanie (filtrowanie, grupowanie i zliczenie) danych na rozproszonych danych (spark sql)\n",
    "# pozniej pobranie do pandasowej ramki\n",
    "hobby_all = spark.sql(f\"SELECT Hobbyist, COUNT(*) AS cnt FROM {table_name} WHERE Hobbyist IS NOT NULL GROUP BY Hobbyist\").toPandas()\n",
    "hobby_men = spark.sql(f\"SELECT Hobbyist, COUNT(*) AS cnt FROM {table_name} WHERE Hobbyist IS NOT NULL AND Gender='Man' GROUP BY Hobbyist\").toPandas()\n",
    "hobby_women = spark.sql(f\"SELECT Hobbyist, COUNT(*) AS cnt FROM {table_name} WHERE Hobbyist IS NOT NULL AND Gender='Woman' GROUP BY Hobbyist\").toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hobby_all.plot.pie(y='cnt', labels=hobby_all['Hobbyist'], title=\"All\", autopct='%.0f')\n",
    "plt.legend(loc=\"lower center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hobby_men.plot.pie(y='cnt', labels=hobby_men['Hobbyist'], title=\"Men\", autopct='%.0f')\n",
    "plt.legend(loc=\"lower center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hobby_women.plot.pie(y='cnt', labels=hobby_women['Hobbyist'], title=\"Women\", autopct='%.0f')\n",
    "plt.legend(loc=\"lower center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\n",
    "\n",
    "hobby_all.plot.pie(y='cnt', labels=hobby_all['Hobbyist'], title=\"All\", ax=axes[0], autopct='%.0f')\n",
    "hobby_men.plot.pie(y='cnt', labels=hobby_men['Hobbyist'], title=\"Men\", ax=axes[1], autopct='%.0f')\n",
    "hobby_women.plot.pie(y='cnt', labels=hobby_men['Hobbyist'], title=\"Women\", ax=axes[2], autopct='%.0f')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wykres liniowy. Zależność między wiekiem a liczbą przepracowanych godzin\n",
    "Interesują nas dla developerzy profesjonaliści (nie hobbiści) w przedziale wiekowym 18-65."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przygotowanie (filtrowanie, grupowanie, wyliczenie sredniej oraz sortowanie) danych na rozproszonych danych (spark sql)\n",
    "# pozniej pobranie do pandasowej ramki\n",
    "\n",
    "age_work = spark.sql(f\"SELECT age, CAST (avg(WorkWeekHrs) AS INT) AS avg FROM {table_name} \\\n",
    "            WHERE WorkWeekHrs IS NOT NULL AND age BETWEEN 18 AND 65 AND hobbyist = 'No' \\\n",
    "            GROUP BY age \\\n",
    "            ORDER BY age ASC\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_work.plot(x='age', y='avg', kind='line')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x=\"age\", y=\"avg\", kind=\"line\", data=age_work);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wykres słupkowy. Pokaż liczbę respondentów na kraj\n",
    "\n",
    "Interesuje nas tylko 10 krajów o najwyższej liczbie respondentow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przygotowanie (grupowanie, zliczenie, sortowanie oraz przyciecie do 10 wyników) danych na rozproszonych danych (spark sql)\n",
    "# pozniej pobranie do pandasowej ramki\n",
    "\n",
    "max_countries = spark.sql(f\"SELECT country, COUNT(*) AS cnt \\\n",
    "                FROM {table_name} \\\n",
    "                GROUP BY country \\\n",
    "                ORDER BY cnt DESC \\\n",
    "                LIMIT 10 \").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_countries.plot.bar(y='cnt', x='country')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"country\", y=\"cnt\", kind=\"bar\",\\\n",
    "            data=max_countries).set_xticklabels(rotation=65)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wykres słupkowy. Średnie zarobki w  krajach w ktorych jest powyzej 1000 respondentów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przygotowanie (filtrowanie, grupowanie, wyliczenie sredniej, filtrowanie po liczności i grup oraz sortowanie) danych na rozproszonych danych (spark sql)\n",
    "# pozniej pobranie do pandasowej ramki\n",
    "\n",
    "country_salary = spark.sql(f\"SELECT country, \\\n",
    "    CAST (avg(ConvertedComp) AS INT) as avg \\\n",
    "    FROM {table_name} \\\n",
    "    WHERE country IS NOT NULL \\\n",
    "    GROUP BY country \\\n",
    "    HAVING COUNT(*) > 1000 \\\n",
    "    ORDER BY avg DESC \").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_salary.plot.barh((\"country\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxplot. Pokaz rozklad pensji w krajach gdzie jest powyzej 1000 respondentów\n",
    "Tutaj będziemy musieli skorzystać z podzapytania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przygotowanie danych na rozproszonych danych (spark sql). Mamy tu do czynienia z podzapytaniem\n",
    "# pozniej pobranie do pandasowej ramki\n",
    "\n",
    "country_comp = spark.sql(f\"SELECT country, CAST(ConvertedComp AS INT) \\\n",
    "                FROM {table_name} \\\n",
    "                WHERE country IN (SELECT country FROM {table_name} GROUP BY country HAVING COUNT(*) > 1000) \\\n",
    "                AND ConvertedComp IS NOT NULL AND ConvertedComp > 0 \\\n",
    "                ORDER BY ConvertedComp desc\").toPandas()\n",
    "country_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_comp.boxplot(column=\"ConvertedComp\", by=\"country\", \\\n",
    "                     showfliers=False, rot=60, meanline=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"country\", y=\"ConvertedComp\", kind=\"box\", \\\n",
    "            showfliers=False, data=country_comp, palette=\"Blues\")\\\n",
    "    .set_xticklabels(rotation=65)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE 3 </span>\n",
    "Narysuj rozklad pensji w zaleznosci od plci."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narysuj wykres popularnosci jezykow programowania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select LanguageWorkedWith from {table_name} where LanguageWorkedWith IS NOT NULL\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Języki programowania są zapisane w pojedynczej komórce. Będzie trzeba je rozdzielić i zliczyć. Tak przygotowane dane dopiero posłużą nam do narysowania wykresu. Wykorzystamy funkcję `posexplode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = spark.sql(f\"select LanguageWorkedWith from {table_name} where LanguageWorkedWith IS NOT NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "langs.select(\n",
    "        posexplode(split(\"LanguageWorkedWith\", \";\")).alias(\"pos\", \"language\")\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs.select(\n",
    "        posexplode(split(\"LanguageWorkedWith\", \";\")).alias(\"pos\", \"language\")).groupBy(\"language\").count().orderBy(\"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs_pd = langs.select(\n",
    "        posexplode(split(\"LanguageWorkedWith\", \";\")).alias(\"pos\", \"language\")).groupBy(\"language\").count().orderBy(\"count\").toPandas()\n",
    "langs_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(8, 9))\n",
    "plt.barh(width=langs_pd[\"count\"], y=langs_pd[\"language\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narysuj wykres popularnosci jezykow wsrod Data Scientists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zdefiniujmy sobie funkcję, która przekształca nam języki do wymaganej przez nas postaci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "def prepare_lang(df, colName='LanguageWorkedWith'):\n",
    "    summary = df.select(posexplode(split(colName, \";\")).alias(\"pos\", \"language\")).groupBy(\"language\").count().orderBy(\"count\")\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs_ds = spark.sql(f\"SELECT LanguageWorkedWith \\\n",
    "                FROM {table_name} \\\n",
    "                WHERE DevType LIKE '%Data scientist%'\")\n",
    "\n",
    "sum_lang = prepare_lang(langs_ds).toPandas()\n",
    "\n",
    "figure(figsize=(8, 9))\n",
    "plt.barh(width=sum_lang[\"count\"], y=sum_lang[\"language\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narysuj wykres którego chcą wykorzystywać w przyszłości Data Scientists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_desired = spark.sql(f\"select LanguageDesireNextYear \\\n",
    "                from {table_name} \\\n",
    "                where DevType like '%Data scientist%'\")\n",
    "\n",
    "sum_lang = prepare_lang(lang_desired, 'LanguageDesireNextYear').toPandas()\n",
    "\n",
    "figure(num=None, figsize=(8, 9), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.barh(width=sum_lang[\"count\"], y=sum_lang[\"language\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narysuj wykres prezentujący liczbę godzin na pracy w zależności od wykształcenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select distinct EdLevel from {table_name}\").show(truncate=False) # jakie są wartości wykształcenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ed_pandas = spark.sql(f\"SELECT EdLevel, WorkWeekHrs FROM {table_name} \\\n",
    "            WHERE WorkWeekHrs BETWEEN 10 AND 80 \\\n",
    "            AND (EdLevel LIKE '%Bachelor%' OR EdLevel LIKE '%Master%' OR EdLevel LIKE '%Other doctoral%')\").toPandas()\n",
    "\n",
    "ed_pandas['EdLevel'] = ed_pandas['EdLevel'].replace('Bachelor’s degree (B.A., B.S., B.Eng., etc.)','Bachelor')\n",
    "ed_pandas['EdLevel'] = ed_pandas['EdLevel'].replace('Master’s degree (M.A., M.S., M.Eng., MBA, etc.)','Master')\n",
    "ed_pandas['EdLevel'] = ed_pandas['EdLevel'].replace('Other doctoral degree (Ph.D., Ed.D., etc.)','Doctor')\n",
    "\n",
    "ed_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"EdLevel\", y=\"WorkWeekHrs\", data=ed_pandas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narysuj wykres wiolinowy pokazujacy rozkład dochodów w zależności od wykształcenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_pay = spark.sql(f\"SELECT EdLevel, CAST (CompTotal AS INT) AS CompTotal FROM {table_name} \\\n",
    "            WHERE CompTotal BETWEEN 0 AND 1000000  \\\n",
    "            AND (EdLevel LIKE '%Bachelor%' OR EdLevel LIKE '%Master%' OR EdLevel LIKE '%Other doctoral%')\").toPandas()\n",
    "\n",
    "ed_pay['EdLevel'] = ed_pay['EdLevel'].replace('Bachelor’s degree (B.A., B.S., B.Eng., etc.)','Bachelor')\n",
    "ed_pay['EdLevel'] = ed_pay['EdLevel'].replace('Master’s degree (M.A., M.S., M.Eng., MBA, etc.)','Master')\n",
    "ed_pay['EdLevel'] = ed_pay['EdLevel'].replace('Other doctoral degree (Ph.D., Ed.D., etc.)','Doctor')\n",
    "\n",
    "ed_pay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"EdLevel\", y=\"CompTotal\", kind=\"boxen\",\n",
    "            data=ed_pay);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐ Narysuj heatmape odwiedzin na StackOverflow dla wybranych krajów ⭐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT DISTINCT SOVisitFreq FROM {table_name}\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_v = spark.sql(f\"SELECT SOVisitFreq, t1.country, COUNT(*)/first(t2.t) AS cnt from {table_name} t1 \\\n",
    "            JOIN (SELECT country, COUNT(*) as t FROM {table_name} GROUP BY country) t2 \\\n",
    "            ON t1.country = t2.country \\\n",
    "            WHERE t1.country IS NOT NULL AND SOVisitFreq IS NOT NULL \\\n",
    "            AND t1.country IN ('Poland', 'United States', 'Russian Federation', 'China', 'India', 'Germany', 'Japan') \\\n",
    "            GROUP BY t1.country, SOVisitFreq\").toPandas()\n",
    "\n",
    "so_v['SOVisitFreq'] = pd.Categorical(so_v['SOVisitFreq'], [\"I have never visited Stack Overflow (before today)\", \"Less than once per month or monthly\", \"A few times per month or weekly\", \"A few times per week\", \"Daily or almost daily\", \"Multiple times per day\"])\n",
    "# so_v.sort_values['SOVisitFreq']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap2_data = pd.pivot_table(so_v, values='cnt', index=['country'], columns='SOVisitFreq')\n",
    "sns.heatmap(heatmap2_data, cmap=\"BuGn\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE 4 </span>\n",
    "* Narysuj wykres słupkowy popularności wykorzystywanych baz danych przez profesjonalnych programistów.\n",
    "* Narysuj wykres kołowy przedstawiający procentowy udział poziomu wykształcenia inz, mgr i dr w grupie respondentów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "notebook_test": {
   "keytab_path": "/data/work/home/ds-lab-testuser1/ds-lab-testuser1.keytab",
   "user": "ds-lab-testuser1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
